---
title: "Untitled"
author: "Alexander Gerber"
date: "19 Oktober 2018"
header-includes:
 - \usepackage{bbm}
 - \usepackage{amsmath}
 - \DeclareMathOperator{\diag}{diag} 
 - \newcommand{\post}{\overline }
 - \newcommand{\prior}{\underline }
output: pdf_document
editor_options: 
  chunk_output_type: console
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## The Model 

The observation equation: 
  \[ y_t = exp\left(\frac{h_t}{2}\right) + \epsilon_t\text{, } \qquad \epsilon_t \sim N(0,1) \]
which can be rewritten to 
\[\log(y_t^2) = h_t + log(\epsilon_t^2)\]
if we approximate $log(\epsilon_t^2)$ by a gaussian mixture rv $\upsilon$ with 
$\upsilon_i|s_i \sim N(d_{s_i}, \sigma^2_{s_i})$ the model is conditional on the mixture component indicator
linear and Gaussian. 

System of state equations:
  \begin{align*}
h_0 &= \mu + \sqrt{\frac{\sigma^2}{1-\phi^2}} u_0\\
h_1 &= X_1 \beta +  (1 - \phi)\mu + \phi h_0 + \sigma u_1\\
\vdots\\
h_T &= X_T \beta +  (1 - \phi)\mu + \phi h_{T-1} + \sigma u_T\\
\end{align*}

where $u_t \sim N(0,1)$.

Rewritten in matrix notation: 
  
  \[H_\phi h = X \beta + \gamma + \Sigma^{\frac{1}{2}} u\]

with 

\[
  H_\phi =  \begin{bmatrix}
  1       &  0      & 0       & \dots  & 0        \\
  -\phi    &  1      & 0       & \dots  & 0        \\
  0       & -\phi   & 1       & \ddots & \vdots   \\
  \vdots  & \ddots  & \ddots  & \ddots & 0        \\
  0       & \dots   & 0       & -\phi  & 1 
  \end{bmatrix}
  \text{, }\quad \gamma = \begin{bmatrix}
  \mu \\
  (1-\phi) \mu \\
  \vdots \\
  (1 -\phi)\mu
  \end{bmatrix}
  \text{, } \quad X =  \begin{bmatrix}
  0       &  \dots  & 0       \\
  x_{11}  &  \dots  & x_{1k}  \\
  \vdots  &  \vdots &  \vdots \\
  x_{T1}  &  \dots & x_{Tk}   \\
  \end{bmatrix}
  \text{, } \quad \beta = \begin{bmatrix} 
  \beta_1 \\
  \vdots \\
  \beta_k
  \end{bmatrix}
  \] 
and $\Sigma = \diag \left(\frac{\sigma^2}{1-\phi^2}, \sigma^2, \ldots, \sigma^2 \right)$.

Conditional on $X$ and the parameters 

\[\prior{\hat{h}} = E[h|\phi, \mu, \beta, X] =  H^{-1}_\phi (X \beta + \gamma) \] 
\[\prior{\Sigma}_h = Var[h|\phi, \sigma^2] =  (H^{\prime}_\phi \Sigma_u^{-1} H_\phi)^{-1} 
   = \begin{bmatrix} 
   \frac{1}{\sigma^2}                      &  \frac{-\phi}{\sigma^2}  & 0                      & \dots   & 0\\
   \frac{-\phi}{\sigma^2} &  \frac{1 + \phi^2}{\sigma^2}                       & \frac{-\phi}{\sigma^2} & \ddots  & \vdots\\
   0                      & \frac{-\phi}{\sigma^2}   & \ddots       & \ddots & 0   \\
   \vdots                 & \ddots  & \ddots  & \frac{1 + \phi^2}{\sigma^2}  & \frac{-\phi}{\sigma^2}        \\
   0                      & \dots   & 0       & \frac{-\phi}{\sigma^2}  & \frac{1}{\sigma^2}
   \end{bmatrix}
   \]


and hence 

\[h|\phi, \mu, \beta, X, \sigma^2 \sim N(\prior{\hat{h}}, \prior{\Sigma}_h)\]

Priors: 
  \[\frac{(\phi + 1)}{2} \sim \mathcal{B}(a_0, b_0)\] and hence
  \[p(\phi) = \frac{1}{2 B(a_0, b_0)} \left(\frac{1 + \phi}{2}\right)^{a_0 - 1} \left(\frac{1 - \phi}{2} \right) ^{b_0-1}\]

  \[\sigma^2 \sim B_\sigma \chi_1^2 = \mathcal{G(1/2, 1/(2 B_{\sigma} )}\] 
  
  \[\beta, \mu \sim N\left(\begin{pmatrix}\prior{\hat{\beta}} \\\prior{\hat{\mu}}\end{pmatrix}, \begin{pmatrix} \prior{\sigma^2_{\beta}} & 0\\0 & \prior{\sigma^2_{\alpha}}\end{pmatrix} \right)\]
  
  
## Gibbs Sampler
  
  
  Sample $h$ (including $h_0$) from 
\[h|y, s, \phi, \mu, \sigma^2, \beta \sim N(\post{\hat{h}}, \post{\Sigma}_h)\]

where the precision matrix

\[\post{\Sigma}_h^{-1} = (\Sigma_y^{-1} + \prior{\Sigma}_h^{-1}) = 
     \begin{bmatrix} 
   \frac{1}{\sigma^2}    &  \frac{-\phi}{\sigma^2}  & 0                      & \dots   & 0\\
   \frac{-\phi}{\sigma^2}                           &  \frac{1 + \phi^2}{\sigma^2} + \frac{1}{\sigma^2_{s_1}} & \frac{-\phi}{\sigma^2} & \ddots  & \vdots\\
   0                                                & \frac{-\phi}{\sigma^2}   & \ddots       & \ddots & 0   \\
   \vdots                                           & \ddots  & \ddots  & \frac{1 + \phi^2}{\sigma^2} + \frac{1}{\sigma^2_{s_{T-1}}}  & \frac{-\phi}{\sigma^2}        \\
   0                                                & \dots   & 0       & \frac{-\phi}{\sigma^2}  & \frac{1}{\sigma^2} + \frac{1}{\sigma^2_{s_T}}
   \end{bmatrix}\]. 
\[ \qquad \post{\hat{h}} = \post{\Sigma}_h (\Sigma_y^{-1} (y - d) + \prior{\Sigma}_h^{-1} \prior{\hat{h}})\] 
and the mean vector 
\begin{align*}
\post{\hat{h}} &= \post{\Sigma}_h (\Sigma_y^{-1} (y - d) + \prior{\Sigma}_h^{-1} \prior{\hat{h}})\\
&= \post{\Sigma}_h (\Sigma_y^{-1} (y - d) + H^{\prime}_\phi \Sigma_u^{-1} (X \beta + \gamma))
\end{align*}

To efficiently sample from this distribuition we exploit the special structure of the precision matrix. 


\[H^{\prime}_\phi \Sigma_u^{-1}  = \begin{bmatrix} 
  \frac{1 - \phi^2}{\sigma^2}     &  \frac{-\phi}{\sigma^2}  & 0                      & \dots   & 0\\
  0                               &  \frac{1}{\sigma^2}                      & \frac{-\phi}{\sigma^2} & \ddots  & \vdots\\
  \vdots                          &  \ddots       & \ddots  & \ddots & 0   \\
  \vdots                          &               &  \ddots  &  \frac{1}{\sigma^2} & \frac{-\phi}{\sigma^2}        \\
  0                               & \dots   & \dots  & 0      & \frac{1}{\sigma^2}
  \end{bmatrix}
  \]


